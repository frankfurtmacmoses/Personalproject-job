"""
# s3 module includes functions for AWS S3 bucket and files

@author: Jason Zhu
@email: jzhu@infoblox.com
@created: 2017-02-01

@notes: The following environment variables are needed for AWS connection

AWS_DEFAULT_REGION=us-west-2
AWS_SECRET_ACCESS_KEY=
AWS_ACCESS_KEY_ID=

"""
import boto3.session as boto3_session
import botocore
import json
import logging
import os
import types

from {{__PROJECT_FOLDER_AS_PYTHON_TOP_MODULE_NAME__}}.utils.logger import get_logger
LOGGER = get_logger(__name__, logging.WARN)

BUCKET_DEFAULT = 'cyber-intel'
PREFIX_PROCESSED = '{{__PROJECT_FOLDER_AS_PYTHON_TOP_MODULE_NAME__}}/processed'
PREFIX_TESTS = '{{__PROJECT_FOLDER_AS_PYTHON_TOP_MODULE_NAME__}}/tests'
PREFIX_DATA = '{{__PROJECT_FOLDER_AS_PYTHON_TOP_MODULE_NAME__}}/data'


def _check_arg_bucket(bucket, resource=None):
    """
    Check if the arg is a valid bucket; otherwise, raise ValueError.
    @param bucket: the bucket name (top-level directory in S3).
    """
    chk = check_bucket(bucket, resource)
    if not chk['okay']:
        LOGGER.error(str(chk['err']))
        raise ValueError("param 'bucket' must be a valid existing S3 bucket")


def _check_arg_as_function(a_func):
    """
    Check if the arg is a function or method; otherwise, raise ValueError
    """
    is_func = isinstance(a_func, types.FunctionType) or isinstance(
        a_func, types.MethodType)
    if not is_func:
        LOGGER.error("param 'a_func' is %s", type(a_func).__name__)
        raise ValueError("param 'a_func' must be a function")


def check_bucket(bucket_name=BUCKET_DEFAULT, resource=None):
    """
    Checks if a S3 bucket exists.

    @param bucket_name: the bucket name (top-level directory in S3).
    @param resource: s3 resource; if it is none, new resource will be created.
    @return: True if bucket exists; otherwise False.
    """
    s3_resource = resource if resource else get_resource()
    result = dict(okay=False, err=None)

    try:
        s3_resource.meta.client.head_bucket(Bucket=bucket_name)
    except botocore.exceptions.ParamValidationError as expv:
        result['err'] = expv
    except botocore.exceptions.ClientError as ex_client_error:
        # If a client error is thrown, then check that it was a 404 error.
        # If it was a 404 error, then the bucket does not exist.
        # error_code = int(e.response['Error']['Code'])
        result['err'] = ex_client_error
    except Exception as ex:
        result['err'] = ex
    else:
        result['okay'] = True
    return result


def check_empty_folder(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Check if an S3 folder (key suffix '/') is empty.

    @param key_name: the s3 key.
    @param bucket: the bucket name (top-level directory in S3).
    @param client: s3 client; if it is none, new client will be created.
    @return: a tuple of (bool, object) where the bool indicates if
             the key named folder is empty, and the object is the s3
             contents (None means an invalid or non-folder key).
    """
    if not str(key_name).endswith('/'):
        return (False, None)

    empty = True
    contents = None
    s3_client = client if client else get_client()
    result = s3_client.list_objects(Bucket=bucket, Prefix=key_name)

    if result:
        contents = result.get('Contents', None)
        if contents:
            for content in contents:
                if content.get('Key') != key_name:
                    empty = False
                    break

    return (empty, contents)


def check_key(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Check if a S3 key exists.
    """
    key = get_key(key_name, bucket, client)
    return key is not None


def check_prefix(prefix, bucket=BUCKET_DEFAULT, client=None):
    """
    Check if a S3 prefix exists.
    """
    s3_client = client if client else get_client()
    results = s3_client.list_objects(Bucket=bucket, Prefix=prefix)
    return 'Contents' in results


def check_size(key, bucket=BUCKET_DEFAULT, client=None):
    """
    Check the size of a s3 file (key) in a bucket.
    """
    s3_client = client if client else get_client()
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key)
        size = response['ContentLength']
        return size > 0
    except Exception as ex:
        LOGGER.debug(ex)
        pass
    return False


def clean_json(contents):
    """
    Clean out trailing commas in JSON string contents.
    """
    import re
    contents = re.sub(',[ \t\r\n]*}', ' }', contents)
    contents = re.sub(',[ \t\r\n]*]', ' ]', contents)
    return contents


def convert_parquet_to_json(parquet_content):
    """
    Convert parquet content to json object

    @param parquet: so called "parquet" content (multi lines JSON string).
    @return: validated JSON content.
    """
    data = []
    for parquet in parquet_content.strip().split('\n'):
        LOGGER.debug('- Parquet: %s\n', parquet)
        parquet = parquet.replace(': NaN', ': null')
        # parquet = clean_json(parquet)  # removing trailing commas
        data.append(json.loads(parquet, 'utf-8'))
    return data


def copy_contents_to_bucket(
        contents, key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Copy a string content to specified key in s3 bucket and
    overwrite original key if it already exists.
    """
    s3_client = client if client else get_client()
    key = get_key(key_name, bucket, client)
    msg = '{} [{}]'.format(key_name, bucket)
    if key is None:
        LOGGER.debug('new key: %s', msg)
    else:
        LOGGER.debug('deleting %s', msg)
        # acl = s3_client.get_object_acl(Bucket=bucket, Key=key_name)
        s3_client.delete_object(Bucket=bucket, Key=key_name)
    LOGGER.debug('put_object: %s', msg)
    try:
        return s3_client.put_object(Body=contents, Bucket=bucket, Key=key_name)
    except Exception as ex:
        LOGGER.error('failure on putting %s:\n%s', msg, ex)
    return None


def copy_to_bucket(
        filename,
        prefix_path=PREFIX_PROCESSED,
        bucket=BUCKET_DEFAULT,
        client=None):
    """
    Upload a local file per path prefix in bucket.

    @param filename: the full path to local filename.
    @param prefix: the prefix (starting under the bucket) of the key name.
    @param bucket: the bucket name (top-level directory in S3).
    @param client: s3 client; if it is none, new client will be created.

    @return: True if upload succeeded; otherwise, False

    example:
        copy_to_bucket('/Users/overload/test.json', 'mined-json')
    """
    if os.path.isfile(filename):
        basename = os.path.basename(filename)
        prefix = prefix_path.strip('/') + '/' + basename
    else:
        LOGGER.error('cannot find: %s', filename)
        return False
    try:
        s3_client = client if client else get_client()
        LOGGER.debug('uploading [%s] to [%s]', filename, prefix)
        with open(filename, 'rb') as data:
            s3_client.upload_fileobj(data, bucket, prefix)
            LOGGER.debug('uploaded: [%s]', prefix)
            return True
    except Exception as ex:
        LOGGER.error(ex)
    return False


def create_key(contents, key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Create a key on s3.
    """
    try:
        s3_client = client if client else get_client()
        return s3_client.put_object(Body=contents, Bucket=bucket, Key=key_name)
    except Exception as ex:
        LOGGER.error('failure on creating %s [%s]:\n%s', key_name, bucket, ex)
        return None


def delete_empty_folder(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Delete an S3 folder (key suffix '/') if it is empty.

    @param client: s3 client; if it is none, new client will be created.
    @return: True if succeeded; otherwise, False.
    """
    if not str(key_name).endswith('/'):
        return False

    empty = True
    contents = None
    s3_client = client if client else get_client()
    result = s3_client.list_objects(Bucket=bucket, Prefix=key_name)

    if result:
        contents = result.get('Contents', None)
        if contents:
            for obj in contents:
                if obj.get('Key', '') != key_name:
                    empty = False
                    break
    if contents and empty:
        LOGGER.info('empty folder: %s [bucket=%s]', key_name, bucket)
        return delete_key(key_name, bucket, client)
    return False


def delete_key(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Deleting a s3 bucket key.

    * dev-note: comparing to using boto3.resource('s3')
        ```
        s3_resource = get_resource()
        s3_resource.Object(bucket, key_name).delete()
        ```
    """
    try:
        s3_client = client if client else get_client()
        LOGGER.info('deleting key: %s [bucket=%s]', key_name, bucket)
        s3_client.delete_object(Bucket=bucket, Key=key_name)
        return True
    except Exception as ex:
        LOGGER.debug(ex)
        return False


def get_client():
    """
    Get S3 client
    note: This function can be customized to accept configurations
    """
    session = boto3_session.Session()
    s3_client = session.client('s3')
    return s3_client


def get_resource():
    """
    Get S3 resource
    note: This function can be customized to accept configurations
    """
    session = boto3_session.Session()
    s3_resource = session.resource('s3')
    return s3_resource


def get_content(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Get content from a s3 file (key_name) in a bucket

    @param client: s3 client; if it is none, new client will be created.
    @return: s3 content.
    """
    s3_client = client if client else get_client()
    try:
        LOGGER.debug('- getting object: %s [bucket="%s"]', key_name, bucket)
        response = s3_client.get_object(Bucket=bucket, Key=key_name)
        size = response['ContentLength']
        if size > 0:
            LOGGER.debug('- reading object: %s [size=%s]', key_name, size)
            contents = response['Body'].read()  # .decode('utf-8')
            return contents
        else:
            LOGGER.debug('- content zero: %s', key_name)
            return ""
    except Exception as ex:
        LOGGER.debug('- content error: %s', key_name)
        LOGGER.debug(ex)

    return None


def get_json_data(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Get JSON data obejct from a s3 file (key_name) in a bucket

    Note: using `json.loads()` to read from string,
          comparing to `json.load()` which read from local file directly
          ```
          with open('filename.json') as data_file:
              data = json.load(data_file)
          ```
    """
    json_content = get_content(key_name, bucket, client)
    if json_content:
        # logger.debug('- Data contents: %s\n', json_content)
        try:
            data = json.loads(json_content)
            LOGGER.debug('- JSON object: %s\n', json.dumps(data))
            return data
        except Exception as ex:
            LOGGER.debug(ex)

    return None


def get_json_files(prefix='', bucket=BUCKET_DEFAULT, client=None):
    """
    This function return a list of json files per prefix in bucket.

    @param prefix: the prefix (starting under the bucket) of the key name.
    @param bucket: the bucket name (top-level directory in S3).
    @param client: s3 client; if it is none, new client will be created.

    @return: a list of keys of *.json files

    example:
        get_json_files('mined-json/2017')
    """
    files = []

    # pylint: disable=unused-argument
    def get_func(key_name, **kwargs):
        """Append key name to file list"""
        files.append(key_name)

    process_json(get_func, prefix, bucket=bucket, client=client)
    return files


def get_key(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Get key object in s3 bucket
    """
    s3_client = client if client else get_client()
    object = s3_client.get_object(Bucket=bucket, Key=key_name)
    return object


def get_keys(prefix='', suffix='/', client=None, **kwargs):
    """
    This function return a list of s3 keys per prefix and suffix in bucket

    @param prefix: the prefix (starting under the bucket) of the key name
    @param suffix: the suffix (ending) of the key name

    @return: a list of s3 keys

    example:
        get_keys('mined-json/2017', '.json')
    """
    keys = []

    # pylint: disable=unused-argument
    def get_func(key_name, **kwargs):
        """Append key to key list"""
        keys.append(key_name)

    process(get_func, prefix, suffix, client, **kwargs)
    return keys


def get_parquet_data(key_name, bucket=BUCKET_DEFAULT, client=None):
    """
    Get parquet data from a s3 file (key_name) in a bucket

    Note: For parquet contents, each line is in valid JSON format
          but the file itself is not.
    """
    parquet_content = get_content(key_name, bucket, client)
    if parquet_content:
        # logger.debug('- Data contents: %s\n', parquet_content)
        try:
            data = convert_parquet_to_json(parquet_content)
            LOGGER.debug('- JSON object: %s\n', json.dumps(data))
            return data
        except Exception as ex:
            LOGGER.debug(ex)
    return None


# pylint: disable=invalid-name
def mv(old_s3key, new_s3key, file_name, s3_bucket=BUCKET_DEFAULT, s3_client=None):
    """
    Rename/move a file from old path to new path with specific s3 bucket.

    @param old_s3key: the original path (prefix of the original key name).
    @param new_s3key: the new path (prefix of the destinated key name).
    @param file_name: the file name (suffix following the path) of the key name.
    @param s3_bucket: the s3 bucket (top-level directory in S3).
    @param s3_client: s3 client; if it is none, new client will be created.

    @return: True if the operation succeeded; otherwise, False
    """
    LOGGER.debug(
        'moving file: "%s" from [%s] to [%s]', file_name, old_s3key, new_s3key)
    old_path = old_s3key + '/' + file_name
    new_path = new_s3key + '/' + file_name

    try:
        client = s3_client if s3_client else get_client()
        source = s3_bucket + '/' + old_path  # the source must include bucket name
        LOGGER.debug('moving [%s] to [%s]', old_path, new_path)
        client.copy_object(Bucket=s3_bucket, CopySource=source, Key=new_path)
        client.delete_object(Bucket=s3_bucket, Key=old_path)
    except Exception:
        return False

    return True


def mv_key(oldkey, newkey, bucket=BUCKET_DEFAULT, resource=None):
    """
    Rename/move an old key to new key within specific s3 bucket

    @param oldkey: the original key name (file name)
    @param newkey: the new key name (file name)
    @param bucket: the bucket name (top-level directory in S3)

    @return: True if the operation succeeded; otherwise, False
    """
    try:
        s3_resource = resource if resource else get_resource()
        source = bucket + '/' + oldkey  # the source must include bucket name
        LOGGER.debug('moving [%s] to [%s]', oldkey, newkey)
        s3_resource.Object(bucket, newkey).copy_from(CopySource=source)
        s3_resource.Object(bucket, oldkey).delete()
        return True
    except Exception:
        return False


def process_func(key, **kwargs):
    """
    default function that can be passed to process()
    """
    import sys
    contents = '{} - {}\n'.format(str(key), str(kwargs))
    sys.stdout.write(contents)


def process_json(a_func=process_func, prefix='', client=None, **kwargs):
    """
    Process all json files in the bucket with specified prefix
    """
    process(a_func, prefix, '.json', client, **kwargs)


def generate_pages(prefix='', client=None, **kwargs):
    """
    This function creates a paginator and yields one page at a time.

    :param prefix: the prefix (starting under the bucket) of the key name
    :return: one page of contents
    """
    bucket = kwargs.get('bucket', BUCKET_DEFAULT)
    # _check_arg_bucket(bucket)

    s3_client = client if client else get_client()
    paginator = s3_client.get_paginator('list_objects')
    parameters = {'Bucket': bucket, 'Prefix': prefix, 'Delimiter': ''}
    p_iterator = paginator.paginate(**parameters)

    for obj in p_iterator.search('Contents'):
        if obj:
            key_name = obj.get('Key', '')
            if key_name.endswith('/'):
                LOGGER.info('- skipping key: %s', key_name)
                continue
            yield obj


# process calls a_func to process all keys in a bucket
def process_keys(a_func=process_func, prefix='', client=None, **kwargs):
    """
    This function calls specified a_func to process all keys with
    any specific prefix in a S3 bucket

    @param a_func: the process function to take each iterated key name
                   the function signature is `def func(obj, **kwargs)`
    @param prefix: the prefix (starting under the bucket) of the key name
    @param client: s3 client; if it is none, new client will be created.
    @param kwargs: the additional parameters for a_func
    @return: N/A

    example:
        process_keys(process_func, 'prefix/tests', bucket='cyber-intel')
    """
    bucket = kwargs.get('bucket', BUCKET_DEFAULT)

    _check_arg_as_function(a_func)
    # _check_arg_bucket(bucket)

    s3_client = client if client else get_client()

    paginator = s3_client.get_paginator('list_objects')
    parameters = {'Bucket': bucket, 'Prefix': prefix, 'Delimiter': ''}
    p_iterator = paginator.paginate(**parameters)
    counts = 0

    for obj in p_iterator.search('Contents'):
        if obj:
            key_name = obj.get('Key', '')
            if key_name.endswith('/'):
                LOGGER.info('- skipping key: %s', key_name)
                delete_empty_folder(key_name, bucket, client)
                continue
            a_func(obj, **kwargs)
            counts += 1

    return counts


# process calls a_func to process all keys by prefix and suffix in a bucket
def process(a_func=process_func, prefix='', suffix='/', client=None, **kwargs):
    """
    This function calls specified a_func to process
    any specific prefix with delimiter (suffix) in a S3 bucket

    @param a_func: the process function to take each iterated key name
                   the function signature is `def func(key_name, **kwargs)`
    @param prefix: the prefix (starting under the bucket) of the key name
    @param suffix: the suffix (ending) of the key name
    @param client: s3 client; if it is none, new client will be created.
    @param kwargs: the additional parameters for a_func
    @return: N/A

    example:
        process(process_func, 'prefix/tests', '.json', bucket='cyber-intel')

    caution:
        the patterns of prefix and suffix match all keys in the bucket, e.g.
            prefix + '/20170116_test.' + suffix
            prefix + '/some/other/folder/foo.' + suffix
            prefix + suffix
        this requires extra step to parse the key name, in order to only
        process "files" directly existing under a prefix "dir/path/"
        since s3 has no hierarchical directory
    """
    bucket = kwargs.get('bucket', BUCKET_DEFAULT)

    if not kwargs.get('chck_bypass', False):
        _check_arg_as_function(a_func)
        _check_arg_bucket(bucket)

    s3_client = client if client else get_client()

    paginator = s3_client.get_paginator('list_objects')
    parameters = {'Bucket': bucket, 'Prefix': prefix, 'Delimiter': suffix}
    iterator = paginator.paginate(**parameters)
    counts = 0

    # logger.info('-- searching: %s', str(parameters))
    if suffix:
        for path in iterator.search('CommonPrefixes'):
            if path is not None:
                # logger.debug('-- path: %s', str(path))
                key = path.get('Prefix', None)
                if key:
                    a_func(key, **kwargs)
                    counts += 1
    else:
        for path in iterator.search('Contents'):
            if path is not None:
                # logger.debug('-- path: %s', str(path))
                key = path.get('Key', None)
                if key:
                    a_func(key, **kwargs)
                    counts += 1
    return counts


if __name__ == '__main__':
    process(
        process_func, prefix=PREFIX_TESTS, suffix='', client=None,
        bucket='cyber-intel-test')
